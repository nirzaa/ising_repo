{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deep_ising.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"9ZAXmPTjg1w6"},"source":["# Import the relevant libraries\n"," \n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","import copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHvOwVN_yeyN"},"source":["# Deep neural network\n","# Here I define the model as was written in the MICE article\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","      super(Net, self).__init__()\n","\n","      self.conv1 = nn.Conv2d(1, 128, kernel_size=1)\n","      self.conv2 = nn.Conv2d(128, 64, kernel_size=1)\n","      self.fc1 = nn.Linear(128, 64)\n","      self.fc2 = nn.Linear(64, 1)\n","\n","\n","      '''\n","      self.conv1 = nn.Conv2d(1, 4, kernel_size=1, stride=1, padding=1)\n","      self.conv2 = nn.Conv2d(4, 4, kernel_size=1, stride=1, padding=1)\n","      #self.fc1 = nn.Linear(120, 4000)\n","      self.fc1 = nn.Linear(48, 4000)\n","      self.fc2 = nn.Linear(4000, 2)\n","      '''\n","\n","    def forward(self, x):\n","      x = self.conv1(x)\n","      x = F.relu(x)\n","      #x = self.conv2(x)\n","      #x = F.relu(x)\n","      x = torch.flatten(x, 1)\n","      x = self.fc1(x)\n","      x = F.relu(x)\n","      x = self.fc2(x)\n","      output = F.log_softmax(x, dim=1)\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kb_JrMODylFX"},"source":["# The loss function - Mutual information\n","# Here we define the loss function of the model as the Mutual information, as we have seen in the MICE article\n","# M_x: is the mutual information as was calculated by the model\n","# M_y: is the mutual information as was calculated theoretically\n","\n","def MyLossFunction(output_train_joint, y_train_joint, output_train_product, y_train_product):\n","  exponent_product = np.exp(output_train_product.detach().numpy())\n","  average_product = np.sum(exponent_product) / len(exponent_product)\n","  average_joint = np.sum(output_train_joint.detach().numpy())/len(output_train_joint.detach().numpy())\n","  M_x = average_joint - np.log(average_product)\n","\n","\n","  #exponent_product = np.exp(y_train_product.detach().numpy())\n","  exponent_product = np.exp(output_train_product.detach().numpy())\n","  average_product = np.sum(exponent_product) / len(exponent_product)\n","\n","  #average_joint = np.sum(y_train_joint.detach().numpy())/len(y_train_joint.detach().numpy())\n","  average_joint = np.sum(output_train_joint.detach().numpy())/len(output_train_joint.detach().numpy())\n","  \n","  \n","  M_y = average_joint - np.log(average_product)\n","\n","  mse = np.mean((M_y - M_x) ** 2)\n","\n","  return mse, -M_y, -M_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yR_AV23jy03P"},"source":["# The train function\n","# Here we train the model with the Adam optimizer while minimizing the loss function - mutual information\n","\n","def train(model, epoch, joint_tensor, product_tensor, MyLossFunction):\n","\n","    PATH = './ising_net.pth'\n","\n","    model.load_state_dict(torch.load(PATH))\n","\n","    y_train_joint = None\n","    y_train_product = None\n","    model.train()\n","    tr_loss = 0\n","    if torch.cuda.is_available():\n","        joint_tensor = joint_tensor.cuda()\n","        product_tensor = product_tensor.cuda()\n","\n","\n","    optimizer.zero_grad()\n","    output_train_joint = model(joint_tensor)\n","    output_train_product = model(product_tensor)\n","\n","    loss_train, M_y, M_x = MyLossFunction(output_train_joint, y_train_joint, output_train_product, y_train_product)\n","\n","    train_losses.append(M_x)\n","\n","    loss_train = torch.from_numpy(np.array(M_x))\n","\n","    loss_train.requires_grad=True\n","\n","    # computing the updated weights of all the model parameters\n","    loss_train.backward()\n","    optimizer.step()\n","    \n","    tr_loss = loss_train.item()\n","    \n","    torch.save(model.state_dict(), PATH)\n","\n","  \n","    '''\n","    if epoch%2 == 0:\n","\n","        print('Epoch : ',epoch+1, '\\t', 'loss :', tr_loss)\n","        print(f'Mutual real: {M_y}, Mutual mine: {tr_loss}')\n","    '''\n","\n","    \n","    return model, train_losses, M_y, M_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oN5hH9Tlg8tb"},"source":["# initialize the ising lattice with randomly zeros and ones\n","\n","random_lattice = torch.randn((2,4))\n","ising_lattice = torch.ones((2,4))\n","ising_lattice[random_lattice < 0.5] = -1\n","sub_sub_ising_product = []\n","sub_sub_ising_joint = []\n","\n","data_len = 5000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sciApVGwtOV4"},"source":["# Finite temperature Metropolis monte carlo\n","# Here we split the ising lattice into subsystems as was written in the MICE article\n","\n","J = 1\n","kT = 3\n","seed = 1234\n","RS = np.random.RandomState(seed)\n","\n","sub_sub_sub_ising_joint = []\n","sub_sub_sub_ising_product = []\n","\n","for data in range(data_len):\n","\n","  sub_sub_ising_joint = []\n","  sub_sub_ising_product = []\n","\n","  sub_ising_joint = []\n","  sub_ising_joint.append((list(torch.split(ising_lattice, 2)))[0][0])\n","  sub_ising_joint.append((list(torch.split(ising_lattice, 2)))[0][1])\n","  places = np.linspace(0, 8, num=8, endpoint=False)\n","  RS.shuffle(places)\n","  ising_flatten = ising_lattice.flatten()\n","  sub_ising_product = []\n","  sub_ising_product.append(torch.tensor([ising_flatten[int(places[0])],\n","                                         ising_flatten[int(places[1])],\n","                                         ising_flatten[int(places[2])],\n","                                         ising_flatten[int(places[3])]]))\n","  places = np.linspace(0, 8, num=8, endpoint=False)\n","  RS.shuffle(places)\n","  sub_ising_product.append(torch.tensor([ising_flatten[int(places[0])],\n","                                         ising_flatten[int(places[1])],\n","                                         ising_flatten[int(places[2])],\n","                                         ising_flatten[int(places[3])]]))\n","  \n","  sub_sub_ising_joint += [list(torch.split(sub_ising_joint[i],2))[j]\n","                          for i in range(2)\n","                          for j in range(2)]\n","  for i in range(2):\n","    for j in range(2):\n","      sub_sub_ising_joint.append(list(torch.split(sub_ising_joint[i],2))[j])\n","  \n","  for i in range(4):\n","    for j in range(2):\n","      sub_sub_sub_ising_joint.append(sub_sub_ising_joint[i][j])\n","\n","  places = np.linspace(0,4,num=4,endpoint=False)\n","  RS.shuffle(places)\n","\n","  sub_sub_ising_product.append(torch.tensor([sub_ising_product[0][RS.randint(0, 4)],\n","                                             sub_ising_product[0][RS.randint(0, 4)]]))\n","  sub_sub_ising_product.append(torch.tensor([sub_ising_product[0][RS.randint(0, 4)],\n","                                             sub_ising_product[0][RS.randint(0, 4)]]))\n","  sub_sub_ising_product.append(torch.tensor([sub_ising_product[1][RS.randint(0, 4)],\n","                                             sub_ising_product[1][RS.randint(0, 4)]]))\n","  sub_sub_ising_product.append(torch.tensor([sub_ising_product[1][RS.randint(0, 4)],\n","                                             sub_ising_product[1][RS.randint(0, 4)]]))\n","\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[0][0])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[0][1])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[1][0])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[1][1])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[2][0])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[2][1])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[3][0])\n","  sub_sub_sub_ising_product.append(sub_sub_ising_product[3][1])\n","  \n","# Here we try to switch one spin value, while using T = const according to the Metropolis Monte Carlo algorithm\n","\n","  places1 = np.linspace(0, 4, num=4, endpoint=False)\n","  RS.shuffle(places1)\n","  places2 = np.linspace(0, 2, num=2, endpoint=False)\n","  RS.shuffle(places2)\n","  r = int(places2[0])\n","  c = int(places1[0])\n","\n","  rowy1 = r + 1\n","  rowy2 = r - 1\n","  coly1 = c + 1\n","  coly2 = c - 1\n","\n","  if rowy1 >= ising_lattice.shape[0]:\n","    rowy1 = 0\n","  \n","  if coly1 >= ising_lattice.shape[1]:\n","    coly1 = 0\n","\n","  # print(rowy1, rowy2, coly1, coly2)\n","\n","  Ergy_old = J * ising_lattice[r][c] * (ising_lattice[r][coly1] + ising_lattice[r][coly2] + ising_lattice[rowy1][c] + ising_lattice[rowy2][c])\n","  Ergy_new = (-1) * J * ising_lattice[r][c] * (ising_lattice[r][coly1] + ising_lattice[r][coly2] + ising_lattice[rowy1][c] + ising_lattice[rowy2][c])\n","  \n","\n","  if Ergy_new < Ergy_old:\n","    ising_lattice[r][c] = ising_lattice[r][c] * (-1)\n","\n","  elif Ergy_new > Ergy_old:\n","    e = np.exp(-Ergy_new/kT)\n","    if np.random.rand(1)[0] < e:\n","      ising_lattice[r][c] = ising_lattice[r][c] * (-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQvUD1WD5aEU"},"source":["# For not changing the entire code:\n","\n","joint_list = copy.deepcopy(sub_sub_sub_ising_joint)\n","\n","product_list = copy.deepcopy(sub_sub_sub_ising_product)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YvEyrut6l29","executionInfo":{"status":"ok","timestamp":1618218849147,"user_tz":-180,"elapsed":22934,"user":{"displayName":"Nir Zadok","photoUrl":"","userId":"14501407210045703304"}},"outputId":"1509e9c5-ebec-477b-fb7c-d5d8f14be696"},"source":["# Neural Net\n","# Here we send our subsystems of joint_tensor and product_tensor into our neural network as was written in the MICE algorithm\n","\n","model = Net()\n","\n","PATH = './ising_net.pth'\n","\n","torch.save(model.state_dict(), PATH)\n","\n","optimizer = Adam(model.parameters(), lr=0.01, weight_decay=0.1)\n","\n","# checking if GPU is available\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","    MyLossFunction = MyLossFunction.cuda()\n","      \n","n_epochs = 25\n","# empty list to store training losses\n","train_losses = []\n","\n","\n","cntr0 = 0\n","cntr1 = 128\n","\n","#n_epochs = 1\n","# training the model\n","for epoch in range(n_epochs):\n","\n","  cntr0 = 0\n","  cntr1 = 128\n","  flag = 0\n","\n","  for batch in range(int((len(sub_sub_sub_ising_product) - (len(sub_sub_sub_ising_product) % 128))/128)):\n","    model, train_losses, M_y, M_x = train(model, epoch, torch.tensor(joint_list[cntr0:cntr1]).view(-1,1,1,1), torch.tensor(product_list[cntr0:cntr1]).view(-1,1,1,1), MyLossFunction)\n","    cntr0 += 128\n","    cntr1 += 128\n","    flag += 1\n","\n","    if flag % 10 == 0:\n","      pass\n","      #print(f'Number : {flag}, Mutual information: {abs(M_x)}')\n","\n","  if epoch%2 == 0:\n","\n","    #pass\n","\n","    #print('Epoch : ',epoch+1)\n","    \n","    print(f'Epoch : {epoch+1}    Mutual information: {abs(M_y)}')\n","    \n","    #print('Epoch : ',epoch+1, '\\t', 'loss :', tr_loss)\n","    #print(f'Mutual real: {M_y}, Mutual mine: {tr_loss}')\n","    #print(f'Mutual real: {M_y}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch : 1    Mutual information: 0.002353936149926697\n","Epoch : 3    Mutual information: 0.002353936149926697\n","Epoch : 5    Mutual information: 0.002353936149926697\n","Epoch : 7    Mutual information: 0.002353936149926697\n","Epoch : 9    Mutual information: 0.002353936149926697\n","Epoch : 11    Mutual information: 0.002353936149926697\n","Epoch : 13    Mutual information: 0.002353936149926697\n","Epoch : 15    Mutual information: 0.002353936149926697\n","Epoch : 17    Mutual information: 0.002353936149926697\n","Epoch : 19    Mutual information: 0.002353936149926697\n","Epoch : 21    Mutual information: 0.002353936149926697\n","Epoch : 23    Mutual information: 0.002353936149926697\n","Epoch : 25    Mutual information: 0.002353936149926697\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"7YhEGaLP5G7m","executionInfo":{"status":"ok","timestamp":1618215016983,"user_tz":-180,"elapsed":6705,"user":{"displayName":"Nir Zadok","photoUrl":"","userId":"14501407210045703304"}},"outputId":"8b2068a2-3c68-4067-e3ab-5266cdf61ac0"},"source":["'''\n","\n","###### Old one #######\n","\n","\n","\n","\n","# Neural Net\n","# Here we send our subsystems of joint_tensor and product_tensor into our neural network as was written in the MICE algorithm\n","\n","\n","joint_tensor = torch.cat(sub_sub_ising_joint,axis=-1).view(-1,2)\n","product_tensor = torch.cat(sub_sub_ising_product,axis=-1).view(-1,2)\n","joint_tensor[joint_tensor == 0] = -1\n","product_tensor[product_tensor == 0] = -1\n","\n","y_train_joint = joint_tensor[:,0] == joint_tensor[:,1]\n","y_train_product = product_tensor[:,0] == product_tensor[:,1]\n","\n","joint_tensor = torch.unsqueeze(joint_tensor, 1)\n","joint_tensor = torch.unsqueeze(joint_tensor, 2)\n","product_tensor = torch.unsqueeze(product_tensor, 1)\n","product_tensor = torch.unsqueeze(product_tensor, 2)\n","\n","model = Net()\n","\n","optimizer = Adam(model.parameters(), lr=0.01, weight_decay=0.1)\n","\n","# checking if GPU is available\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","    MyLossFunction = MyLossFunction.cuda()\n","      \n","n_epochs = 25\n","# empty list to store training losses\n","train_losses = []\n","\n","# training the model\n","for epoch in range(n_epochs):\n","    model, train_losses, M_y, M_x = train(model, epoch, joint_tensor, product_tensor, MyLossFunction)\n","\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\n###### Old one #######\\n\\n\\n\\n\\n# Neural Net\\n# Here we send our subsystems of joint_tensor and product_tensor into our neural network as was written in the MICE algorithm\\n\\n\\njoint_tensor = torch.cat(sub_sub_ising_joint,axis=-1).view(-1,2)\\nproduct_tensor = torch.cat(sub_sub_ising_product,axis=-1).view(-1,2)\\njoint_tensor[joint_tensor == 0] = -1\\nproduct_tensor[product_tensor == 0] = -1\\n\\ny_train_joint = joint_tensor[:,0] == joint_tensor[:,1]\\ny_train_product = product_tensor[:,0] == product_tensor[:,1]\\n\\njoint_tensor = torch.unsqueeze(joint_tensor, 1)\\njoint_tensor = torch.unsqueeze(joint_tensor, 2)\\nproduct_tensor = torch.unsqueeze(product_tensor, 1)\\nproduct_tensor = torch.unsqueeze(product_tensor, 2)\\n\\nmodel = Net()\\n\\noptimizer = Adam(model.parameters(), lr=0.01, weight_decay=0.1)\\n\\n# checking if GPU is available\\nif torch.cuda.is_available():\\n    model = model.cuda()\\n    MyLossFunction = MyLossFunction.cuda()\\n      \\nn_epochs = 25\\n# empty list to store training losses\\ntrain_losses = []\\n\\n# training the model\\nfor epoch in range(n_epochs):\\n    model, train_losses, M_y, M_x = train(model, epoch, joint_tensor, product_tensor, MyLossFunction)\\n\\n'"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"oEJJTCoPIJIx"},"source":[""],"execution_count":null,"outputs":[]}]}